{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f8ec95",
   "metadata": {},
   "source": [
    "# 1. Prepare data\n",
    "\n",
    "Test the writing and reading performance on GeoLake.\n",
    "\n",
    "\n",
    "We are going to use a Portaxi dataset which has 2m records. You can find it here: https://star.cs.ucr.edu/?portotaxi#center=41.1636,-8.5872&zoom=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba858681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "wget --no-verbose https://star.cs.ucr.edu/datasets/portotaxi/download.geojson.gz -O - | gzip -d > /home/iceberg/data/portotaxi.geojson\n",
    "chmod 777 /home/iceberg/data/portotaxi.geojson\n",
    "\n",
    "# this takes a while, file has about 3,5GB and trasnfer speeds are pretty low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e7f09",
   "metadata": {},
   "source": [
    "We can also validate if the portotaxi file was correctly downloaded by run the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "[ \"$(md5sum /home/iceberg/data/portotaxi.geojson | cut -d ' ' -f1)\" = \"bd11ae6f439da60c9a2768c6f87af5bc\" ] && echo \"MD5 hash matches expected value.\" || echo \"MD5 hash does not match expected value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb4381",
   "metadata": {},
   "source": [
    "We need to convert the file from multiline to line delimited so we can read it with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c122b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import ijson\n",
    "import jsonlines\n",
    "\n",
    "num_rows_missing_geometry = 0\n",
    "input_path = \"/home/iceberg/data/portotaxi.geojson\"\n",
    "output_path = \"/home/iceberg/data/portotaxi.geojsonl\"\n",
    "\n",
    "with open(input_path) as f, jsonlines.open(output_path, mode=\"w\") as writer:\n",
    "    for feature in ijson.items(f, \"features.item\", use_float=True):\n",
    "        if feature.get(\"geometry\"):\n",
    "            writer.write(feature)\n",
    "        else:\n",
    "            num_rows_missing_geometry += 1\n",
    "print(f\"Finished writing file: {output_path}. {num_rows_missing_geometry} rows were missing geometry and were skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e77adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "chmod 777 /home/iceberg/data/portotaxi.geojsonl\n",
    "# make the files accessible from outside docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72225b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "\n",
    "# You can adjust memory and other spark settings here\n",
    "\n",
    "launcher.num_executors = 1\n",
    "launcher.executor_cores = 8\n",
    "launcher.driver_memory = '16g'\n",
    "launcher.executor_memory = '16g'\n",
    "launcher.conf.set(\"spark.driver.maxResultSize\",\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# if no enough memory, you can split the file and only read the first 10k records\n",
    "# uncomment line below to prepare smaller file\n",
    "#pip install geojsplit && cd /home/iceberg/data/ && geojsplit -n 1 --geometry-count 10000 portotaxi.geojson && ls -lh /home/iceberg/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b066cb6",
   "metadata": {},
   "source": [
    "Create DataFrame out of line-delimited file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"geometry\", StringType)\n",
    "    .add(\"properties\", new StructType()\n",
    "        .add(\"CALL_TYPE\", StringType)\n",
    "        .add(\"DAY_TYPE\", StringType)\n",
    "        .add(\"MISSING_DATA\", BooleanType)\n",
    "        .add(\"ORIGIN_CALL\", StringType)\n",
    "        .add(\"ORIGIN_STAND\", StringType)\n",
    "        .add(\"TAXI_ID\", LongType)\n",
    "        .add(\"TIMESTAMP\", StringType)\n",
    "        .add(\"TRIP_ID\", LongType)\n",
    "    )\n",
    "    .add(\"type\", StringType)\n",
    "\n",
    "val df = spark\n",
    "    .read\n",
    "    .schema(schema)\n",
    "    .json(\"/home/iceberg/data/portotaxi.geojsonl\")\n",
    "    .selectExpr(\"properties.*\", \"ST_GeomFromGeoJSON(geometry) as geometry\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416db77",
   "metadata": {},
   "source": [
    "Create temp view `portotaxi` with columns that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".select(\n",
    "    \"TRIP_ID\",\n",
    "    \"CALL_TYPE\",\n",
    "    \"ORIGIN_STAND\",\n",
    "    \"TAXI_ID\",\n",
    "    \"TIMESTAMP\",\n",
    "    \"DAY_TYPE\",\n",
    "    \"MISSING_DATA\",\n",
    "    \"geometry\"\n",
    ")\n",
    ".repartition(10)\n",
    ".cache\n",
    ".createOrReplaceTempView(\"portotaxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM portotaxi\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b82eb",
   "metadata": {},
   "source": [
    "# 2. Benchmark of Parquet Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bab7a0",
   "metadata": {},
   "source": [
    "## Create Tables\n",
    "\n",
    "Create table with different geo-encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c249eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val geoEncodings = Seq(\"nested-list\", \"wkb-bbox\", \"wkb\")\n",
    "val tables = Seq(\"portotaxi_nested_list\", \"portotaxi_wkb_bbox\", \"portotaxi_wkb\")\n",
    "\n",
    "tables.zip(geoEncodings).foreach(x => {\n",
    "    val sql = s\"\"\"CREATE TABLE IF NOT EXISTS demo.db.${x._1}\n",
    "    (\n",
    "      TRIP_ID LONG,\n",
    "      CALL_TYPE STRING,\n",
    "      ORIGIN_STAND STRING,\n",
    "      TAXI_ID INTEGER,\n",
    "      TIMESTAMP STRING,\n",
    "      DAY_TYPE STRING,\n",
    "      MISSING_DATA BOOLEAN,\n",
    "      geometry GEOMETRY\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.parquet.geometry.encoding' = '${x._2}')\n",
    "    \"\"\"\n",
    "    println(sql)\n",
    "    spark.sql(sql)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bf4ab",
   "metadata": {},
   "source": [
    "## Writing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26200f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.foreach(tb => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"INSERT INTO demo.db.${tb} SELECT * FROM portotaxi\")\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table ${tb}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sql = tables.map(t => s\"\"\"\n",
    "(SELECT '${t}' as table, \n",
    "    summary['total-records'] as total_records,\n",
    "    round(summary['total-files-size'] / 1024 / 1024, 2) as file_size_in_mb\n",
    " FROM demo.db.${t}.snapshots)\n",
    "\"\"\").reduce(_ + \" UNION \" + _)\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb97635",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val bbox = \"POLYGON ((-8.6079 41.1489, -8.6089 41.1472, -8.6066 41.1470, -8.6061 41.1483, -8.6079 41.1489))\"\n",
    "tables.foreach(t => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"SELECT count(*) FROM demo.db.${t} WHERE ST_Within(geometry, IcebergSTGeomFromText('${bbox}'))\").show()\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table ${t}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade369b",
   "metadata": {},
   "source": [
    "# 3. Benchmark of Partitions\n",
    "\n",
    "\n",
    "Create tables with different partition resolution: 3, 7, 11, 15, 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacffdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val resolutions = Seq(3, 7, 11, 15, 19)\n",
    "resolutions.foreach(r => {\n",
    "    val sql = s\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS demo.db.portotaxi_xz${r}\n",
    "    (\n",
    "      TRIP_ID LONG,\n",
    "      CALL_TYPE STRING,\n",
    "      ORIGIN_STAND STRING,\n",
    "      TAXI_ID INTEGER,\n",
    "      TIMESTAMP STRING,\n",
    "      DAY_TYPE STRING,\n",
    "      MISSING_DATA BOOLEAN,\n",
    "      geometry GEOMETRY\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (xz2(geometry, ${r}))\n",
    "    TBLPROPERTIES ('write.parquet.geometry.encoding' = 'nested-list')    \n",
    "    \"\"\"\n",
    "    spark.sql(sql)\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"INSERT INTO demo.db.portotaxi_xz${r} SELECT * FROM portotaxi\")\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table demo.db.portotaxi_xz${r}: ${(t1 - t0) / 1000.0}s\")\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3773c20",
   "metadata": {},
   "source": [
    "Number of partitions and data files in each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20202abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sql = resolutions.map(t => s\"(SELECT ${t} as resolution, summary['changed-partition-count'] as partitions,summary['total-data-files'] as total_data_files FROM demo.db.portotaxi_xz${t}.snapshots)\").reduce(_ + \" UNION \" + _) + \" ORDER BY resolution\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b56f5d",
   "metadata": {},
   "source": [
    "Reading speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions.foreach(t => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"SELECT count(*) FROM demo.db.portotaxi_xz${t} WHERE ST_Within(geometry, ST_GeomFromText('${bbox}'))\").show()\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on resolutions ${t}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
