{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f8ec95",
   "metadata": {},
   "source": [
    "# 1. Prepare data\n",
    "\n",
    "Test the writing and reading performance on GeoLake.\n",
    "\n",
    "\n",
    "We are going to use a Portaxi dataset which has 2m records. You can find it here: https://star.cs.ucr.edu/?portotaxi#center=41.1636,-8.5872&zoom=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba858681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "wget https://star.cs.ucr.edu/datasets/portotaxi/download.geojson.gz -O - | gzip -d > /home/iceberg/data/portotaxi.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "ls -lh /home/iceberg/data/portotaxi.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72225b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "\n",
    "# The file is large(3.5G), run this cell if you have enough memory\n",
    "\n",
    "launcher.num_executors = 1\n",
    "launcher.executor_cores = 8\n",
    "launcher.driver_memory = '16g'\n",
    "launcher.executor_memory = '16g'\n",
    "launcher.conf.set(\"spark.driver.maxResultSize\",\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# if no enough memory, you can split the file and only read the first 10k records\n",
    "pip install geojsplit && cd /home/iceberg/data/ && geojsplit -n 1 --geometry-count 10000 portotaxi.geojson && ls -lh /home/iceberg/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.wololo.geojson.GeoJSONFactory\n",
    "import org.wololo.jts2geojson.GeoJSONReader\n",
    "import org.wololo.geojson.Feature\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "\n",
    "def readGeojson(filePath: String): DataFrame = {\n",
    "    val colnames = Seq(\"TRIP_ID\", \"CALL_TYPE\", \"ORIGIN_STAND\", \"TAXI_ID\", \"TIMESTAMP\", \"DAY_TYPE\", \"MISSING_DATA\",  \"geometry\")\n",
    "    val geoJsonString = spark.read.textFile(filePath).collect().mkString\n",
    "    val mapper = new ObjectMapper()\n",
    "    val it = mapper.readTree(geoJsonString).get(\"features\").iterator()\n",
    "    var features = Seq[Feature]()\n",
    "    while (it.hasNext()) {\n",
    "        val nextFea = it.next()\n",
    "        try {\n",
    "            features = features :+ GeoJSONFactory.create(nextFea.toString).asInstanceOf[Feature]\n",
    "        } catch {\n",
    "             case e: Exception => null\n",
    "        }\n",
    "    }\n",
    "    val rows = features.map(feature => {\n",
    "        val reader = new GeoJSONReader\n",
    "        (\n",
    "    \t\tfeature.getProperties.get(\"TRIP_ID\").asInstanceOf[Long]\n",
    "          \t,feature.getProperties.get(\"CALL_TYPE\").asInstanceOf[String]\n",
    "    \t\t,feature.getProperties.get(\"ORIGIN_STAND\").asInstanceOf[String]\n",
    "    \t\t,feature.getProperties.get(\"TAXI_ID\").asInstanceOf[Integer]\n",
    "    \t\t,feature.getProperties.get(\"TIMESTAMP\").asInstanceOf[String]\n",
    "    \t\t,feature.getProperties.get(\"DAY_TYPE\").asInstanceOf[String]\n",
    "    \t\t,feature.getProperties.get(\"MISSING_DATA\").asInstanceOf[Boolean]\n",
    "    \t\t,reader.read(feature.getGeometry)\n",
    "    \t)\n",
    "    })\n",
    "    sc.parallelize(rows).toDF(colnames: _*)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fullFile = \"/home/iceberg/data/portotaxi_xaaaa.geojson\" // change to \"portotaxi.geojson\" if you have enough memory\n",
    "val df = readGeojson(fullFile).repartition(10)\n",
    "df.cache\n",
    "df.createOrReplaceTempView(\"portotaxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM portotaxi\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b82eb",
   "metadata": {},
   "source": [
    "# 2. Benchmark of Parquet Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bab7a0",
   "metadata": {},
   "source": [
    "## Create Tables\n",
    "\n",
    "Create table with different geo-encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c249eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val geoEncodings = Seq(\"nested-list\", \"wkb-bbox\", \"wkb\")\n",
    "val tables = Seq(\"portotaxi_nested_list\", \"portotaxi_wkb_bbox\", \"portotaxi_wkb\")\n",
    "\n",
    "tables.zip(geoEncodings).foreach(x => {\n",
    "    val sql = s\"\"\"CREATE TABLE IF NOT EXISTS demo.db.${x._1}\n",
    "    (\n",
    "      TRIP_ID LONG,\n",
    "      CALL_TYPE STRING,\n",
    "      ORIGIN_STAND STRING,\n",
    "      TAXI_ID INTEGER,\n",
    "      TIMESTAMP STRING,\n",
    "      DAY_TYPE STRING,\n",
    "      MISSING_DATA BOOLEAN,\n",
    "      geometry GEOMETRY\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.parquet.geometry.encoding' = '${x._2}')\n",
    "    \"\"\"\n",
    "    println(sql)\n",
    "    spark.sql(sql)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bf4ab",
   "metadata": {},
   "source": [
    "## Writing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26200f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.foreach(tb => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"INSERT INTO demo.db.${tb} SELECT * FROM portotaxi\")\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table ${tb}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sql = tables.map(t => s\"\"\"\n",
    "(SELECT '${t}' as table, \n",
    "    summary['total-records'] as total_records,\n",
    "    round(summary['total-files-size'] / 1024 / 1024, 2) as file_size_in_mb\n",
    " FROM demo.db.${t}.snapshots)\n",
    "\"\"\").reduce(_ + \" UNION \" + _)\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb97635",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val bbox = \"POLYGON ((-8.6079 41.1489, -8.6089 41.1472, -8.6066 41.1470, -8.6061 41.1483, -8.6079 41.1489))\"\n",
    "tables.foreach(t => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"SELECT count(*) FROM demo.db.${t} WHERE ST_Within(geometry, IcebergSTGeomFromText('${bbox}'))\").show()\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table ${t}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade369b",
   "metadata": {},
   "source": [
    "# 3. Benchmark of Partitions\n",
    "\n",
    "\n",
    "Create tables with different partition resolution: 3, 7, 11, 15, 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacffdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val resolutions = Seq(3, 7, 11, 15, 19)\n",
    "resolutions.foreach(r => {\n",
    "    val sql = s\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS demo.db.portotaxi_xz${r}\n",
    "    (\n",
    "      TRIP_ID LONG,\n",
    "      CALL_TYPE STRING,\n",
    "      ORIGIN_STAND STRING,\n",
    "      TAXI_ID INTEGER,\n",
    "      TIMESTAMP STRING,\n",
    "      DAY_TYPE STRING,\n",
    "      MISSING_DATA BOOLEAN,\n",
    "      geometry GEOMETRY\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (xz2(geometry, ${r}))\n",
    "    TBLPROPERTIES ('write.parquet.geometry.encoding' = 'nested-list')    \n",
    "    \"\"\"\n",
    "    spark.sql(sql)\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"INSERT INTO demo.db.portotaxi_xz${r} SELECT * FROM portotaxi\")\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on table demo.db.portotaxi_xz${r}: ${(t1 - t0) / 1000.0}s\")\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3773c20",
   "metadata": {},
   "source": [
    "Number of partitions and data files in each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20202abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sql = resolutions.map(t => s\"(SELECT ${t} as resolution, summary['changed-partition-count'] as partitions,summary['total-data-files'] as total_data_files FROM demo.db.portotaxi_xz${t}.snapshots)\").reduce(_ + \" UNION \" + _) + \" ORDER BY resolution\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b56f5d",
   "metadata": {},
   "source": [
    "Reading speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions.foreach(t => {\n",
    "    val t0 = System.currentTimeMillis()\n",
    "    spark.sql(s\"SELECT count(*) FROM demo.db.portotaxi_xz${t} WHERE ST_Within(geometry, ST_GeomFromText('${bbox}'))\").show()\n",
    "    val t1 = System.currentTimeMillis()\n",
    "    println(s\"time cost on resolutions ${t}: ${(t1 - t0) / 1000.0}s\")\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
